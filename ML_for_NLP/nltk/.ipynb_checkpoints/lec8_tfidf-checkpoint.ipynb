{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i will follow the same problem statement as in lecture 6 and 7\n",
    "# earlier approaches of BOW and one-hot-encoding didn't take importance of words in a sentence into account.\n",
    "# however, tfidf approach solves this problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Term Freq (TF)} = \\frac{\\text{No. of repetation of word in a sentence}}{\\text{No. of words in a sentence}}$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\\text{Inverse Document Freq (IDF)} = \\log_e \\left( \\frac{\\text{No. of sentences}}{\\text{No. of sentences containing the word}} \\right)$$\n",
    "\n",
    "Finally, we get a matrix by multiplying individual entries with the correponding pairs.\n",
    "\n",
    "Clearly, we have a matrix that assigns importance to individual words in a sentence, where words that are repeated across all sentences are given no importance, whereas previously, they were still assigned some value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoding': 'Windows-1252', 'confidence': 0.7269493857068697, 'language': ''}\n"
     ]
    }
   ],
   "source": [
    "# slight issue with the encoding of the csv file\n",
    "import chardet\n",
    "\n",
    "with open('datasets/bag_of_words_dataset/spam.csv', 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     v1                                                 v2 Unnamed: 2  \\\n",
      "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
      "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
      "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
      "\n",
      "  Unnamed: 3 Unnamed: 4  \n",
      "0        NaN        NaN  \n",
      "1        NaN        NaN  \n",
      "2        NaN        NaN  \n",
      "3        NaN        NaN  \n",
      "4        NaN        NaN  \n"
     ]
    }
   ],
   "source": [
    "messages = pd.read_csv(\n",
    "    filepath_or_buffer=\"datasets/bag_of_words_dataset/spam.csv\",\n",
    "    delimiter=',',\n",
    "    #     names=[\"label\", \"message\"],\n",
    "    encoding='Windows-1252',\n",
    ")\n",
    "\n",
    "print(messages.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     label                                            message\n",
      "0      ham  Go until jurong point, crazy.. Available only ...\n",
      "1      ham                      Ok lar... Joking wif u oni...\n",
      "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3      ham  U dun say so early hor... U c already then say...\n",
      "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
      "...    ...                                                ...\n",
      "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
      "5568   ham              Will ÃŒ_ b going to esplanade fr home?\n",
      "5569   ham  Pity, * was in mood for that. So...any other s...\n",
      "5570   ham  The guy did some bitching but I acted like i'd...\n",
      "5571   ham                         Rofl. Its true to its name\n",
      "\n",
      "[5572 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# remove unnamed colums and clean the data\n",
    "messages = messages.drop(columns=[\"Unnamed: 2\",\"Unnamed: 3\",\"Unnamed: 4\"], axis=1)\n",
    "messages.rename(columns={\"v1\": \"label\", \"v2\": \"message\"}, inplace=True)\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import regular expression and perform cleansing\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# further clean the data \n",
    "# messages -> replace all the special characters -> lemmatization -> store the corpus as a list\n",
    "corpus_list = []\n",
    "\n",
    "for i in range(len(messages)):\n",
    "    ind_review = re.sub(pattern=\"[^a-zA-Z]\", repl=\" \", string=messages['message'][i])\n",
    "#     print(ind_review)\n",
    "    ind_review = ind_review.lower()\n",
    "    ind_review = ind_review.split()\n",
    "    ind_review = [wordnet_lemmatizer.lemmatize(word) for word in ind_review if word not in set(stopwords.words(\"english\"))]\n",
    "    ind_review = \" \".join(ind_review)\n",
    "    corpus_list.append(ind_review)\n",
    "\n",
    "# lets look at the final data\n",
    "print(type(corpus_list))\n",
    "# print(corpus_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will be using sklearn BOW approach\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create BOW with default n_gram range of (1,1)\n",
    "# there are multiple important options: stopwords, lower_case, N-gram\n",
    "tfid_generator = TfidfVectorizer(max_features=100) \n",
    "X_train = tfid_generator.fit_transform(corpus_list).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(5572, 100)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train))\n",
    "print(X_train.shape)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'go': np.int64(21), 'great': np.int64(25), 'got': np.int64(24), 'wat': np.int64(89), 'ok': np.int64(58), 'free': np.int64(17), 'win': np.int64(93), 'text': np.int64(77), 'txt': np.int64(85), 'say': np.int64(68), 'already': np.int64(0), 'think': np.int64(80), 'life': np.int64(37), 'hey': np.int64(28), 'week': np.int64(91), 'back': np.int64(5), 'like': np.int64(38), 'still': np.int64(73), 'send': np.int64(70), 'friend': np.int64(18), 'prize': np.int64(63), 'claim': np.int64(8), 'call': np.int64(6), 'mobile': np.int64(48), 'co': np.int64(9), 'home': np.int64(30), 'want': np.int64(88), 'today': np.int64(82), 'cash': np.int64(7), 'day': np.int64(13), 'reply': np.int64(65), 'www': np.int64(96), 'right': np.int64(66), 'take': np.int64(75), 'time': np.int64(81), 'message': np.int64(45), 'com': np.int64(10), 'oh': np.int64(57), 'yes': np.int64(99), 'make': np.int64(43), 'way': np.int64(90), 'dont': np.int64(15), 'miss': np.int64(47), 'ur': np.int64(87), 'going': np.int64(22), 'da': np.int64(12), 'lor': np.int64(40), 'meet': np.int64(44), 'really': np.int64(64), 'know': np.int64(33), 'lol': np.int64(39), 'love': np.int64(41), 'amp': np.int64(2), 'let': np.int64(36), 'work': np.int64(94), 'yeah': np.int64(97), 'tell': np.int64(76), 'thanks': np.int64(78), 'uk': np.int64(86), 'please': np.int64(61), 'msg': np.int64(50), 'see': np.int64(69), 'pls': np.int64(62), 'need': np.int64(52), 'nokia': np.int64(55), 'tomorrow': np.int64(83), 'hope': np.int64(31), 'well': np.int64(92), 'lt': np.int64(42), 'gt': np.int64(26), 'get': np.int64(19), 'ask': np.int64(3), 'morning': np.int64(49), 'happy': np.int64(27), 'sorry': np.int64(72), 'give': np.int64(20), 'new': np.int64(53), 'find': np.int64(16), 'year': np.int64(98), 'later': np.int64(35), 'good': np.int64(23), 'come': np.int64(11), 'said': np.int64(67), 'hi': np.int64(29), 'babe': np.int64(4), 'im': np.int64(32), 'much': np.int64(51), 'stop': np.int64(74), 'one': np.int64(59), 'night': np.int64(54), 'service': np.int64(71), 'dear': np.int64(14), 'would': np.int64(95), 'thing': np.int64(79), 'last': np.int64(34), 'min': np.int64(46), 'number': np.int64(56), 'also': np.int64(1), 'phone': np.int64(60), 'tone': np.int64(84)}\n"
     ]
    }
   ],
   "source": [
    "print(tfid_generator.vocabulary_)\n",
    "# NOTICE: 'go': np.int64(930), here 930 refers to the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 100)\n"
     ]
    }
   ],
   "source": [
    "# lets try to change the n_gram range\n",
    "tfid_generator = TfidfVectorizer(max_features=100, ngram_range=(1,2)) # combination of bigram and unigram\n",
    "X_train = tfid_generator.fit_transform(corpus_list).toarray()\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'go': np.int64(21), 'great': np.int64(25), 'got': np.int64(24), 'wat': np.int64(89), 'ok': np.int64(58), 'free': np.int64(17), 'win': np.int64(93), 'text': np.int64(77), 'txt': np.int64(85), 'say': np.int64(68), 'already': np.int64(0), 'think': np.int64(80), 'life': np.int64(37), 'hey': np.int64(28), 'week': np.int64(91), 'back': np.int64(5), 'like': np.int64(38), 'still': np.int64(73), 'send': np.int64(70), 'friend': np.int64(18), 'prize': np.int64(63), 'claim': np.int64(8), 'call': np.int64(6), 'mobile': np.int64(48), 'co': np.int64(9), 'home': np.int64(30), 'want': np.int64(88), 'today': np.int64(82), 'cash': np.int64(7), 'day': np.int64(13), 'reply': np.int64(65), 'www': np.int64(96), 'right': np.int64(66), 'take': np.int64(75), 'time': np.int64(81), 'message': np.int64(45), 'com': np.int64(10), 'oh': np.int64(57), 'yes': np.int64(99), 'make': np.int64(43), 'way': np.int64(90), 'dont': np.int64(15), 'miss': np.int64(47), 'ur': np.int64(87), 'going': np.int64(22), 'da': np.int64(12), 'lor': np.int64(39), 'meet': np.int64(44), 'really': np.int64(64), 'know': np.int64(33), 'love': np.int64(40), 'amp': np.int64(2), 'let': np.int64(36), 'work': np.int64(94), 'yeah': np.int64(97), 'tell': np.int64(76), 'thanks': np.int64(78), 'uk': np.int64(86), 'please': np.int64(61), 'msg': np.int64(50), 'see': np.int64(69), 'pls': np.int64(62), 'need': np.int64(52), 'nokia': np.int64(55), 'tomorrow': np.int64(83), 'hope': np.int64(31), 'well': np.int64(92), 'lt': np.int64(41), 'gt': np.int64(26), 'lt gt': np.int64(42), 'get': np.int64(19), 'ask': np.int64(3), 'morning': np.int64(49), 'happy': np.int64(27), 'sorry': np.int64(72), 'give': np.int64(20), 'new': np.int64(53), 'find': np.int64(16), 'year': np.int64(98), 'later': np.int64(35), 'good': np.int64(23), 'come': np.int64(11), 'said': np.int64(67), 'hi': np.int64(29), 'babe': np.int64(4), 'im': np.int64(32), 'much': np.int64(51), 'stop': np.int64(74), 'one': np.int64(59), 'night': np.int64(54), 'service': np.int64(71), 'dear': np.int64(14), 'would': np.int64(95), 'thing': np.int64(79), 'last': np.int64(34), 'min': np.int64(46), 'number': np.int64(56), 'also': np.int64(1), 'phone': np.int64(60), 'tone': np.int64(84)}\n"
     ]
    }
   ],
   "source": [
    "# now, lets look at the vocabulary\n",
    "print(tfid_generator.vocabulary_)\n",
    "# NOTICE: 'please call': np.int64(322)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting a configuration where i can see the matrix clearly\n",
    "import numpy as np\n",
    "np.set_printoptions(edgeitems=30, linewidth=100000, \n",
    "    formatter=dict(float=lambda x: \"%.3g\" % x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.435 0 0 0.461 0.544 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.55 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.456 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.473 0 0 0 0 0 0 0 0.492 0 0 0 0 0 0 0 0.572 0 0 0 0 0 0]\n",
      " [0.464 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.886 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.486 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.659 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.574 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0.389 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.412 0 0 0 0 0 0 0 0 0 0.351 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.335 0 0 0 0 0 0 0 0 0 0 0 0.365 0 0 0.385 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.403 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0.283 0 0.85 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.444 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0.189 0 0 0.275 0 0 0 0 0 0 0 0.481 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.811 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one should alos change the hyperparameters like ngram_range or max_features to increase the accuracy!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-nlp_preprocess]",
   "language": "python",
   "name": "conda-env-.conda-nlp_preprocess-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
